{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** убрать все внешние картинки **\n",
    "\n",
    "** для всего кода прямо над клеткой ставить формулы**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Линейная регрессия\n",
    "\n",
    "## Почему мы изучаем ее, а не глубокие нейронные сети?\n",
    "\n",
    "В данном уроке мы изучим основные концепции и идеи, которые широко применяются в машинном обучении. Линейная регрессия достаточно простая для понимания, и она до сих пор применяется во многих задачах.\n",
    "\n",
    "** Добавить больше зачем они нужны и где применяеются сейчас. Хотя бы эксель и ClickHouse.**\n",
    "\n",
    "** Рассказать что есть из коробки. Но важно понимать инструмент. **\n",
    "\n",
    "** Расказать про sklearn **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "\n",
    "Представим, что мы хотим определить цену в одном доме квартиры на основании площади квартиры. \n",
    "У нас есть база данных полученная от риэлтерской компании. \n",
    "В данном случае нам нужно найти зависимость между площадью квартиры и ценой квартиры. \n",
    "\n",
    "То есть, нужно найти функцию $f(X) = y$, где $X$ это площадь квартиры, а $y$ это цена на квартиру. Это и есть задача регрессии.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример: \n",
    "*Нам говорят, у квартиры площадь 45 квадратных метров. Мы берем это значение, помещаем в нашу функцию и получаем значение.* \n",
    "\n",
    "$f(45) = 10 000 000$\n",
    "\n",
    "*И мы говорим в ответ: она будет стоит 10 млн рублей.*\n",
    "\n",
    "** Больше про функции **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Другой пример регрессии.** У нас есть интернет сайт и мы хотим предсказать сколько посититлей у нас будет в в определенное время, при этом у нас есть статистика посещений сайта от времени. Наша задача - используя имеющиеся данные о загруженности в прошлом, предсказать количество поситителей в интересующиее нас время. То есть по имеющимся данным построить зависимость интересующией нас переменной от каких то других параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае задача регрессии - это по входным данным найти число в каком-то *непрерывном* диапазоне. Например: $(-1, 1)$ или же $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим подробнее пример с ценами на квартиру.\n",
    "Давайте загрузим. После этого в вектре $X$ у нас будет площадь квартиры в $м^2$, а в вектре $y$ цена на квартиру. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Добавить bias к данным и уменьшить таргет на миллион, что бы уменшить числа **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from regression_helper import * # Подгружаем функции для визуализации\n",
    "import numpy as np              # Подгруджаем библиотеку NumPy\n",
    "\n",
    "X, y = get_data()               # Загружаем данные в X и y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Чуть больше показать данные **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим эти точки на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, y)      # Строим диаграму с точками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_X(X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_y(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X_i$ в тексте - это ***X[i]*** в коде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что чем больше площадь квартиры, тем выше ее цена. Можно сделать вывод, что между этими данными есть зависимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим мы предполагаем, что данная зависимость может описаться линейной функцией вида $y = kX$, где $X$ это площадь квартиры, а $y$ ее цена, $k$ - коэффициент который мы попытаемся подобрать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы предполагаем, что данную зависимость можно описать такой функцией, это наша гипотеза. \n",
    "Давайте нанесем на график несколько линейных функций с разным коэффициентом что бы убедится, что такой вид функции подходит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_slope(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем эту функцию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед каждой функций будем писать что конкретно мы реализуем.\n",
    "\n",
    "Самая простая реализация в самом простом виде выглядит так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого элемента $X_i$ массива $X$ реализовать фукнуию $f(X_i) = kX_i$.\n",
    "\n",
    "* На входе массив $X$\n",
    "\n",
    "* На выходе массив со значениями $f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_realization(X, k):\n",
    "    result = []            # Массив \n",
    "    for x in X:\n",
    "        result.append(k*x)\n",
    "    return np.array(result)\n",
    "\n",
    "print(dummy_realization(X, k=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это не самая лучшая реализация. \n",
    "\n",
    "Можно воспользоваться возможностью питона и сделать код покороче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого элемента $X_i$ массива $X$ реализовать фукнуию $f(X_i) = kX_i$.\n",
    "\n",
    "* На входе массив $X$\n",
    "\n",
    "* На выходе массив со значениями $f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def less_dummy_realization(X, k):\n",
    "    return np.array([k*x for x in X])\n",
    "\n",
    "print(less_dummy_realization(X, k=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждого элемента $X_i$ массива $X$ реализовать фукнуию $f(X_i) = kX_i$.\n",
    "\n",
    "* На входе массив $X$\n",
    "\n",
    "* На выходе массив со значениями $f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X, k):\n",
    "    return k*X\n",
    "\n",
    "print(f(X=X, k=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом, эта функция также работает и для одного элемента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f(X=X[0], k=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разница во времени выполнения на 100000 элементов в ndarray\n",
    "\n",
    "Функция                | Среднее время (микросекунд) \n",
    "-----------------------|-----------------------------\n",
    "dummy_realization      | 31400                       \n",
    "less_dummy_realization | 13100                       \n",
    "f                      | 70                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция ошибки\n",
    "\n",
    "У нас есть гипотезы. Но как численно определить какая из них лучшая?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для это введем функцию ошибку, также известную как функцию потерь. Функция ошибки - численное значение того, какая разница между функцией которой мы получили и данными. Обозначается функция потерь как $Loss$, $L$ или $J$. \n",
    "\n",
    "Лучше всего не использовать значение $L$. \n",
    "\n",
    "$L$ - часто используется как обозначение для функции правдопадобия, которая часто используется в машинном обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте визуализируем ошибки для наших гипотез."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_and_error(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Каждая формула на отдельной строке **\n",
    "\n",
    "Пусть у нас есть функция (наша модель):  \n",
    "\n",
    "$\\widetilde{y} = f(X) = kX$\n",
    "\n",
    "То есть, $\\widetilde{y}$ является предсказанными нами значениями для $X$. А настоящие значения будут равны $y$. Тогда ошибку нашего предсказания на $i$-ом примере $\\widetilde{y}_i$ можно посчитать, как: \n",
    "\n",
    "$error = \\widetilde{y}_i - y_i$\n",
    "\n",
    "Если нам не важно в большую или в меньшую сторону мы ошибаемся, можем избавиться от знака, взяв либо модуль либо квадрат ошибки. Для начала давайте возьмем квадрат. О модуле мы поговорим во втором уроке регрессии. Тогда ошибка $j(k)$ на i-ом примере будет равна:\n",
    "\n",
    "$j(k) = (\\widetilde{y}_i - y_i)^2$\n",
    "\n",
    "$j(k) = (kX_i - y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "for i in range(X.shape[0]): \n",
    "    diff = f(X[i], k) - y[i]\n",
    "    print(f\"Разница на примере {i} равна {diff:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "for i in range(X.shape[0]): \n",
    "    diff_quad = (f(X[i], k) - y[i])**2\n",
    "    print(f\"Квадрат разницы на примере {i} равен {diff_quad:.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "print(f(X, k) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "print((f(X, k) - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но это на одном примере. А на всех?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разбить формулы **\n",
    "\n",
    "Тогда мы можем посчитать среднюю ошибку $J(k)$ на всех примерах:\n",
    "\n",
    "$J(k) = \\dfrac{1}{N} \\sum_{i=0}^{N}{(\\widetilde{y}_i - y_i)^2}$ \n",
    "\n",
    "$J(k) = \\dfrac{1}{N} \\sum_{i=0}^{N}{(f(X_i) - y_i)^2}$\n",
    "\n",
    "$J(k) = \\dfrac{1}{N} \\sum_{i=0}^{N}{(kX_i - y_i)^2} $\n",
    "\n",
    "Где $N$ - это колличество примеров, $X_i$ - это площадь i-oй квартиры, $y_i$ - цена для i-oй квартиры, $\\widetilde{y}_i$ - предсказанная цена для i-oй квартиры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Немного про нотацию*** \n",
    "\n",
    "Функция потерь формально зависит от входных данных, реальных выходных данных, вида функции и параметров этой функции. \n",
    "\n",
    "То есть, $J(X, y, f, k)$. Но обычно, для краткости, мы предполагаем, что мы используем текущие $X$, $y$ и $f$ и пишем, что функция потерь зависит только от параметров функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для входных данных массива $X$ и реального выходного значения $y$ реализовать функцию ошибки. \n",
    "\n",
    "На входе:\n",
    "\n",
    "* Коэффициент $k$ функции $f$\n",
    "\n",
    "* Массив входных значений $X$\n",
    "\n",
    "* Массив реальных (верных) выходных значений $y$\n",
    "\n",
    "На выходе:\n",
    "\n",
    "* На выходе значение фуникции ошибки $J(k)$\n",
    "\n",
    "$J(k) = \\dfrac{1}{N} \\sum_{i=0}^{N}{(kX_i - y_i)^2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(k, X, y):\n",
    "    \n",
    "    N = X.shape[0]                    # получаем размер вектора столбца\n",
    "    # или N = len(X)\n",
    "   \n",
    "    J_for_sample = (k*X - y)**2\n",
    "    J = np.sum(J_for_sample) / N      \n",
    "    \n",
    "    # или J = np.mean((k*X - y)**2)\n",
    "    return J\n",
    "\n",
    "k = 10\n",
    "print(loss_function(k, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_and_J(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Давайте теперь визуализируем всю функцию ошибки для всех функция значений $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объяснить что это не аналитическая форма. Расказать что это  создано фором и можно сделать только для данного случая **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_J(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Производная\n",
    "\n",
    "Наша задачи - это минимизации функции ошибки. То есть, нужно найти такое $k$, для которого фунция $J(k)$ имеет минимальное значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того что бы это сделать придется немного потерпеть и вспомнить математику. \n",
    "\n",
    "Давайте повторим что такое *производная функции*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Производная функции $f(x)$ записывается как $f'(x)$ или же как $\\dfrac{d f(x)}{dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама производная это то, насколько значение функции меняется в зависимости от изменения входного значения. \n",
    "\n",
    "В данном случае для какой-то точки $x_0$ производную можно рассматривать \n",
    "\n",
    "$f'(x_0) = \\dfrac{d f(x_0)}{dx} $\n",
    "\n",
    "$f'(x_0) = \\dfrac{f(x_0+\\Delta x) - f(x_0)}{\\Delta x}$\n",
    "\n",
    "$\\Delta x \\rightarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на функцию $f(x) = x^2 + 1.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В какой-то точке $x_0$ производная будет равна:\n",
    "\n",
    "$f'(x_0) = \\dfrac{f(x_0+\\Delta x) - f(x_0)}{\\Delta x}$\n",
    "\n",
    "\n",
    "$f(x_0+\\Delta x) = (x_0 + \\Delta x)^2 + 1.5 = x_0^2 + 2x_0\\Delta x + \\Delta x^2 + 1.5$\n",
    "\n",
    "\n",
    "$ \\dfrac{f(x_0+\\Delta x) - f(x_0)}{\\Delta x} = \\dfrac{x_0^2 + 2x_0\\Delta x + \\Delta x^2 + 1.5 - (x_0^2 + 1.5) }{\\Delta x} = \\dfrac{2x_0\\Delta x + \\Delta x^2}{\\Delta x}$\n",
    "\n",
    "\n",
    "$\\dfrac{2x_0\\Delta x + \\Delta x^2}{\\Delta x} = 2x_0 + \\Delta x$\n",
    "\n",
    "И если мы будем счиать $\\Delta x$ очень маленьким числом, тогда \n",
    "\n",
    "$f'(x_0) = 2x_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1.4\n",
    "derivation(x0=x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = -1.3\n",
    "derivation(x0=x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно считать, что производная в точке показывает \"скорость\" изменения функции. Она положительна если функция растет и отрицательно, когда убывает.\n",
    "\n",
    "Еще ее можно интерпретировать как угол наклона $\\alpha$ касательной.\n",
    "\n",
    "Обычно пишут что $f'(x) = tg(\\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим повевдение производной в точках перегиба.\n",
    "\n",
    "Точки перегиба - это, например, когда функция принимает максимальные и минимальные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivation(x0=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точках перегиба, производная равна 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расмотрим функцию и ее производную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_func_and_der(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При взятии производной сложной функции использоваться chain rule:\n",
    "\n",
    "**Chain рул нужно расписать до интуции **\n",
    "\n",
    "$u = \\phi(x)$\n",
    "\n",
    "$y= f(u)$\n",
    "\n",
    "$\\frac{dy}{dx} = \\frac{df(u)}{du} \\cdot \\frac{du(x)}{dx}$\n",
    "\n",
    "Производная суммы/разности равна сумме/разности производных:\n",
    "\n",
    "** Расписать все правила и добавить примеры произоводных для этих правил  **\n",
    "\n",
    "$(f_1(x) + f_2(x))' = f_1'(x) + f_2'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 1 ###\n",
    "\n",
    "$y = sin(3x - 5)$\n",
    "\n",
    "$u = 3x - 5$\n",
    "\n",
    "$y = sin(u)$\n",
    "\n",
    "$\\frac{dy}{dx} = \\frac{d sin(u)}{du} \\cdot \\frac{d(3x - 5)}{dx}$ \n",
    "\n",
    "\n",
    "$\\frac{d sin(u)}{du} = cos(u)$\n",
    "\n",
    "$\\frac{d(3x - 5)}{dx} = 3$\n",
    "\n",
    "\n",
    "$\\frac{dy}{dx} = 3cos(3x - 5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 2 ###\n",
    "\n",
    "$y = (10x^2 + x)^3$\n",
    "\n",
    "$u = 10x^2 + x$\n",
    "\n",
    "$y = u^3$\n",
    "\n",
    "$\\frac{dy}{dx} = \\frac{d  u^3}{du} \\cdot \\frac{d(10x^2 + x)}{dx}$ \n",
    "\n",
    "\n",
    "$\\frac{d  u^3}{du} = 3u^2$\n",
    "\n",
    "$\\frac{d(10x^2 + x)}{dx} = 20x + 1$\n",
    "\n",
    "$\\frac{dy}{dx} = 3u^2 \\cdot(20x + 1) = 3(10x^2 + x)^2 \\cdot(20x + 1) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, в нашем случае:\n",
    "\n",
    "$ J'(k) = \\dfrac{dJ(k)}{dk} = \\dfrac{\\dfrac{1}{N}\\sum_{i=1}^{N}{(y_i - \\widetilde{y}_i)^2}}{dk} $\n",
    "\n",
    "$\\dfrac{dJ(k)}{dk} = 2 \\cdot \\dfrac{1}{N}\\sum_{i=1}^{N} (kX_i - y_i)\\dfrac{d(kX_i - y_i)}{dk} $\n",
    "\n",
    "$\\dfrac{dJ(k)}{dk} =  \\dfrac{2}{N} \\sum_{i=1}^{N} (kX_i - y_i)X_i$ \n",
    "          \n",
    "И для нахождения минимума нужно приравнять производную к нулю и решаем уравнение относительно k. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$ \\dfrac{dJ(k)}{dk} = 0$\n",
    "\n",
    "Решив уравнение, мы получим значение для $k=$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я думаю, что вы знаете как можно найти значение минимума для данной функции. Нужно взять производную функции ошибки и приравнять ее к нулю. $J'(k) = \\frac{dJ(k)}{dk} = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_and_hyp(X, y, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Но в реальных приложениях не всегда возможно решить это уравнение (далее я расскажу поподробнее о таких ситуациях). \n",
    "\n",
    "Поэтому, познакомимся с таким алгоритмом как градиентный спуск. \n",
    "\n",
    "Данный алгоритм является одним самых распространенных алгоритмов в машинном обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** показать значение производной в точках со слайдером **\n",
    "\n",
    "** Полный график производной **\n",
    "\n",
    "** Показать что производная слишком большая и нельзя так прыгать используя только ее ** \n",
    "\n",
    "** Показать что чем больше ошибка, тем больше производная, для MSE **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib import cm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from ipywidgets import interact, IntSlider,  FloatSlider\n",
    "\n",
    "\n",
    "\n",
    "def der_J(X, y, k):\n",
    "    return np.mean((k*X - y)*X)*2\n",
    "\n",
    "def plot_all_J_with_der(X, y):\n",
    "    k_slider = FloatSlider(min=0, max=2, step=0.1, value=0.1)\n",
    "\n",
    "    @interact(k=k_slider)\n",
    "    def plot_all_J_with_der_inner(k):\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.title(\"Значение ошибки\")\n",
    "        plt.ylabel(\"Значение функции потерь\")\n",
    "        plt.xlabel(\"Значение коэффициента $k$\")\n",
    "        ks = np.linspace(-1, 2.65, 100)\n",
    "        plt.plot(ks, [J(tmp_k, X, y) for tmp_k in ks], color='black')\n",
    "        plt.scatter(k, J(k, X, y), s=50, marker='+', label=\"Текущее значение\")\n",
    "        \n",
    "        k_0 = (k - J(k, X, y) / der_J(X, y, k))\n",
    "        k_next = 5 if k > k_0 else -5\n",
    "        \n",
    "        plt.plot([ k_0, k_next], [0, J(k, X, y) + der_J(X, y, k)*(k_next - k)], label=\"Текущее значение\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.xlim([-1, 3])\n",
    "        plt.ylim([0, 0.15])\n",
    "        plt.show() \n",
    "\n",
    "plot_all_J_with_der(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск\n",
    "\n",
    "** переписать менее математично **\n",
    "\n",
    "** объяснить альфу **\n",
    "\n",
    "Как вызнаете, значение производной в точке равно значению тангенса угла наклона касательной в данной точке или же показывает растет ли функция или убывает.\n",
    "\n",
    "Использую эту информацию мы можем понять где находится минимум и изменить значение $k$ в сторону минимума. \n",
    "\n",
    "* Если производная положительная (как касательная в точке 190000), то нам нужно уменьшать значение $k$. \n",
    "* Если производная отрицательная (как касательная в точке 180000), то нам нужно увеличить значение $k$.  \n",
    "\n",
    "Таким образом сам алгоритм градиентного спуска можно описать следующим образом.\n",
    "\n",
    "* Выбираем случайное значение для $k$\n",
    "* Повторить пока не сойдется:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $k_{new} = k - \\alpha \\cdot \\frac{d}{dk} J(k)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $k = k_{new}$\n",
    "\n",
    "Где $\\alpha$ это коэффициент, который мы выбреем. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_function(k, X, y):\n",
    "    N = X.shape[0]       \n",
    "    grad = 2*np.sum((k * X - y) * X) / N                       \n",
    "    return grad   \n",
    "\n",
    "k = 170000\n",
    "print(gradient_function(k, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(k_init, X, y, alpha, iters):\n",
    "    k = k_init\n",
    "    for i in range (0, iters):\n",
    "        k = k - (alpha * gradient_function(k, X, y))\n",
    "    return k\n",
    "\n",
    "k_init = 170000 \n",
    "alpha = 0.0001\n",
    "iters = 10\n",
    "gradient_descent(k_init, X, y, alpha, iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналитически подсчитаный k = 185072.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим как ведет себя алгоритм с различными $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** гифка или слйдер с лучшей визулизацией **\n",
    "\n",
    "** добавить функию ошики и одновременно результурующий график. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Traice(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Немного усложним пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве гипотезы мы использовали функцияю вида $f(X) = kX$.\n",
    "\n",
    "Многие из вас могли заметить, что это не совсем линейная функция. Линейная фунция на самом деле выглядит так:\n",
    "\n",
    "$f(X) = kX + b$.\n",
    "\n",
    "Давайте изменим гипотезу и теперь будем использовать \"настоящую\" линейную функцию.\n",
    "\n",
    "Но давайте немного изменим обозначения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regression_helper\n",
    "\n",
    "X, y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(X, k, b):\n",
    "    return k*X + b\n",
    "\n",
    "print(f(X=X, k=10, b=-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перепишем функцию ошибки:\n",
    "\n",
    "$\\hat{y} = f(X) = kX + b$. То есть, $\\hat{y}$ является предсказанными нами значениями для $X$. \n",
    "А настоящие значения будут равны $y$. \n",
    "\n",
    "Тогда ошибка на одном примере равна $j(k, b)$ на i-ом примере будет равна $j(k, b) = (\\hat{y_i} - y_i)^2 = ((kX_i + b) - y_i)^2$\n",
    "\n",
    "$J(k, b) = \\frac{1}{N}\\sum_{i=0}^{N}{(\\hat{y_i} - y_i)^2}= \\frac{1}{N} \\sum_{i=0}^{N}{((kX_i + b) - y_i)^2} $\n",
    "\n",
    "То есть теперь у нас ошибка зависит не только от $k$, но и от $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearn_loss_function(k, b, X, y):\n",
    "    \n",
    "    N = X.shape[0]       # получаем размер вектора столбца\n",
    "    # или N = len(X)\n",
    "   \n",
    "    J = np.sum(((k*X + b) - y)**2)/N\n",
    "    # или J = np.mean((k*X - y)**2)/N\n",
    "    return J\n",
    "\n",
    "k = 0\n",
    "print(linearn_loss_function(k=k, b=10, X=X, y=y) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_linear_loss_in_3d(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте введем понятие градиента.\n",
    "\n",
    "Градиент фунцкии $\\phi(x_0, x_1, \\ldots x_N)$ от N переменных это \n",
    "\n",
    "N значений: $\\frac{\\delta \\phi(x_0, x_1, \\ldots x_N)}{\\delta x_0}, \\frac{\\delta \\phi(x_0, x_1, \\ldots x_N)}{\\delta x_1}, \\ldots \\frac{\\delta \\phi(x_0, x_1, \\ldots x_N)}{\\delta x_N}$\n",
    "\n",
    "По сути - это координаты вектора, со значениями производных по разным осям. \n",
    "\n",
    "\n",
    "$\\phi(x, y) = z$ \n",
    "\n",
    "$grad = \\frac{\\delta \\phi(x, y)}{\\delta x} \\cdot \\overleftarrow{i} + \\frac{\\delta \\phi(x, y)}{\\delta y} \\cdot \\overleftarrow{j}$\n",
    "\n",
    "\n",
    "<img src=\"img/grad.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "** добавить рисунок вида сверху и прямо показать стрелки с градинтами для точки**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример градиента от функции двух переменых**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда определим нашу функцию ошибки от параметров $k, b$:\n",
    "\n",
    "$J(k, b) = \\frac{1}{N}\\sum_{i=1}^{N}{(\\hat{y_i} - y_i)^2}= \\frac{1}{N} \\sum_{i=1}^{N}{( (k X + b)  - y_i)^2}$\n",
    "\n",
    "Для нахождения этих коэффициентов также используем градиентный спуск. Но теперь нам необходимо найти производную от функции ошибки для каждого параметра.\n",
    "\n",
    "$\\frac{\\delta  J(k, b)}{\\delta b} = \\frac{2}{N}\\sum_{i=1}^{N} ((k X_i + b)  - y_i)$ \n",
    "\n",
    "$\\frac{\\delta J(k, b)}{\\delta k} = \\frac{2}{N}\\sum_{i=1}^{N} ((k X_i + b)  - y_i)X_i$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда алгоритм градиентного спуска можно описать следующим образом:\n",
    "\n",
    "* Выбираем случайное значение для $k$ и $b$\n",
    "* Повторить пока не сойдется:\n",
    "\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $b_{new} = b  - \\alpha \\cdot \\frac{\\delta }{\\delta b} J(k, b)$ \n",
    "    \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $k_{new} = k - \\alpha \\cdot \\frac{\\delta }{\\delta k} J(k, b)$ \n",
    "    \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $b = b_{new}$, \n",
    "    \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  $k = k_{new}$\n",
    "    \n",
    "\n",
    "Где $\\alpha$ это коэффициент, который мы выбираем. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot_data(X, y, k, b):\n",
    "    #create_base_plot()\n",
    "    plt.scatter(X, y,  color='black', marker=\"o\", s=50)   \n",
    "    \n",
    "    plt.plot([0, 0.5], [b, k*0.5 + b])\n",
    "    #plt.ylim([0, 15])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "X, y =  get_data()\n",
    "print(X)\n",
    "print(y)\n",
    "k = 0.2\n",
    "b = 0.4\n",
    "plot_data(X, y, k, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_linear_loss_in_3d(X, y):    \n",
    "    angles1 = IntSlider(min=0, max=90, step=1, value=90, description='Вертикальное')\n",
    "    angles2 = IntSlider(min=0, max=180, step=1, value=0, description='Горизонтальное')\n",
    "\n",
    "    @interact(angle1=angles1, angle2=angles2)\n",
    "    def plot_loss(angle1, angle2):\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        ax = fig.gca(projection='3d')\n",
    "\n",
    "        # Make data.2.28\n",
    "        ks = np.linspace(-4, 8, 50)#(-30, 30, 20)\n",
    "        bs = np.linspace(-50, 50, 50)#(-100, 100, 20)#(-5, 3, 20)\n",
    "        ks, bs = np.meshgrid(ks, bs)\n",
    "\n",
    "        Z = np.zeros_like(ks)\n",
    "        for i in range(len(ks)):\n",
    "            for j in range(len(bs)):\n",
    "                Z[i, j] = linearn_loss_function(ks[i, j], bs[i, j], X, y)\n",
    "\n",
    "        ax.set_xlabel('Значение параметра $k$')\n",
    "        ax.set_ylabel('Значение параметра $b$')\n",
    "        ax.set_zlabel('Функция ошибки')\n",
    "        \n",
    "        surf = ax.plot_surface(ks, bs, Z,   linewidth=1, antialiased=False, cmap=cm.viridis_r)\n",
    "        #surf = ax.plot_wireframe(ks, bs, Z,   cmap=cm.coolwarm)\n",
    "        ax.view_init(angle1, angle2)\n",
    "        \n",
    "        #ax.set_xlim(-30, 30)\n",
    "        #ax.set_ylim(0, 20)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "plot_linear_loss_in_3d(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_function(k, b, X, y):\n",
    "    N = X.shape[0]\n",
    "    tmp = ( (k * X + b) - y ) *X\n",
    "    #print(tmp)\n",
    "    grad_b = 2*np.sum( ((k * X + b) - y)) / N                       \n",
    "    grad_k = 2*np.sum( ((k * X + b) - y) * X) / N                       \n",
    "    return np.array([grad_k, grad_b])\n",
    "\n",
    "k = 5\n",
    "b = 1\n",
    "print(gradient_function(k, b, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def gradient_descent(k_init, b_init, X, y, alpha, iters):\n",
    "    k = k_init\n",
    "    b = b_init\n",
    "    for i in range (0, iters):\n",
    "        gradients = gradient_function(k, b, X, y)\n",
    "        k = k - (alpha * gradients[0])\n",
    "        b = b - (alpha * gradients[1])\n",
    "        print(i, k, b, gradients, linearn_loss_function(k, b, X, y) )\n",
    "        #plot_data(X, y, k, b)\n",
    "    \n",
    "    return k, b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k_init = -5\n",
    "b_init = 0\n",
    "alpha = 0.75\n",
    "iters = 250\n",
    "\n",
    "\n",
    "k, b = gradient_descent(k_init, b_init, X, y, alpha, iters)\n",
    "#print(k, b)\n",
    "plot_data(X, y, k, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lin_grad_linear(X, y, alpha, iters=20, k_init=0, b_init=0):    \n",
    "    angles1 = IntSlider(min=0, max=90, step=1, value=90, description='Вертикальное')\n",
    "    angles2 = IntSlider(min=0, max=180, step=1, value=0, description='Горизонтальное')\n",
    "    \n",
    "    k = None\n",
    "    b = None\n",
    "    \n",
    "    ks = np.linspace(-5, 5, 20)\n",
    "    bs = np.linspace(-5, 5, 20)\n",
    "    ks, bs = np.meshgrid(ks, bs)\n",
    "    Z = np.zeros_like(ks)\n",
    "    for i in range(len(ks)):\n",
    "        for j in range(len(ks)):\n",
    "            Z[i, j] = linearn_loss_function(ks[i, j], bs[i, j], X, y)\n",
    "    \n",
    "    k = k_init\n",
    "    b = b_init\n",
    "    \n",
    "    all_k = [k]\n",
    "    all_b = [b]\n",
    "    Js = [linearn_loss_function(k, b, X, y)]\n",
    "    \n",
    "    for i in range(iters):\n",
    "        y_h = k*X + b - y \n",
    "        k = k - alpha * 2*sum(y_h * X) / len(X)\n",
    "        b = b - alpha * 2*sum(y_h) / len(X)\n",
    "        all_k.append(k)\n",
    "        all_b.append(b)\n",
    "        Js.append(linearn_loss_function(k, b, X, y))\n",
    "    \n",
    "    @interact(angle1=angles1, angle2=angles2)    \n",
    "    def plot_trace(angle1, angle2):\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        ax = fig.gca(projection='3d')\n",
    "\n",
    "        ax.set_xlabel('Значение коэффициента $k$')\n",
    "        ax.set_ylabel('Значение коэффициента $b$')\n",
    "        ax.set_zlabel('Функция ошибка')\n",
    "        surf = ax.plot_wireframe(ks, bs, Z,  cmap=cm.coolwarm)\n",
    "\n",
    "        ax.scatter(all_k[0], all_b[0], Js[0], c=\"yellow\")\n",
    "        ax.scatter(all_k[1:-1], all_b[1:-1], Js[1:-1], c=\"blue\")\n",
    "        ax.scatter(all_k[-1], all_b[-1], Js[-1], c=\"Red\")\n",
    "        ax.view_init(angle1, angle2)\n",
    "        plt.show()\n",
    "        \n",
    "    return k, b\n",
    "\n",
    "a=0.25\n",
    "lin_grad_linear(X, y, alpha=a, iters=500, k_init=-4, b_init=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "03fc38044a514408b9de41fa70bf122f": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "19b7b5447a594ae1a20b764ac13118cc": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    },
    "28304c0dfbab44f5a851af84bf5947d3": {
     "views": [
      {
       "cell_index": 21
      }
     ]
    },
    "3f700e837fd046ddb14ee8776ac8bc04": {
     "views": [
      {
       "cell_index": 31
      }
     ]
    },
    "50d7c06a111e43c1860a5b10a576bfd3": {
     "views": [
      {
       "cell_index": 23
      }
     ]
    },
    "a74e340d7bb24d9bb020ad7dd982e764": {
     "views": [
      {
       "cell_index": 19
      }
     ]
    },
    "d5593dc80c0445c7ae64c7da2d37f72f": {
     "views": [
      {
       "cell_index": 5
      }
     ]
    },
    "d644b9b546b849c6b17acb3be1f1952b": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    },
    "eac5158d5b6c45c4afa73b58b8b0e58a": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "f4cec2743faf43ad9b8e7c828c6ee76a": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
